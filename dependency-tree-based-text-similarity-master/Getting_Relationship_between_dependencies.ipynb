{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import MSELoss, Module\n",
    "import spacy\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, batch_size):\n",
    "    \n",
    "    no_of_batches = len(data) // batch_size\n",
    "    \n",
    "    for n in range(0, len(data), no_of_batches):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            x1 = data.sent_1.iloc[n:n + no_of_batches].values\n",
    "            x2 = data.sent_2.iloc[n:n + no_of_batches].values\n",
    "            Y = data.score.iloc[n:n + no_of_batches].values\n",
    "        \n",
    "        except IndexError:\n",
    "            \n",
    "            x1 = data.sent_1.iloc[n:].values\n",
    "            x2 = data.sent_2.iloc[n:].values\n",
    "            Y = data.score.iloc[n:].values\n",
    "    \n",
    "    yield x1, x2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSimilarity():\n",
    "    \n",
    "    def __init__(self, tags_dict = None, correlation_matrix = None):\n",
    "        \n",
    "        self._tags = self._get_tags_dict() if tags_dict is None else tags_dict\n",
    "        self._no_of_tags = len(self._tags)\n",
    "#         TODO: initialize weights to identity, random, zeros\n",
    "        self._tag_correlation_matrix = np.identity(self._no_of_tags) if correlation_matrix is None else correlation_matrix\n",
    "        #self._parser = spacy.load(\"en\")\n",
    "        self._parser = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    def _get_tags_dict(self):\n",
    "        \n",
    "        with open(\"data/tags.json\",\"r\") as fl:\n",
    "            tags = json.load(fl)\n",
    "            \n",
    "        return tags\n",
    "    \n",
    "    def _similarity_word(self, pair_A, pair_B):\n",
    "\n",
    "        #getting head and dependent texts \n",
    "        head_a, head_b = pair_A[0].text, pair_B[0].text\n",
    "        dep_a, dep_b = pair_A[2].text, pair_B[2].text\n",
    "\n",
    "        if head_a == head_b:\n",
    "            head = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for heads\n",
    "                head_a, head_b = wn.synsets(head_a)[0], wn.synsets(head_b)[0]\n",
    "                \n",
    "#                 TODO:Change similarity method\n",
    "                #path based similarity (Li et. al) for head\n",
    "                head = head_a.path_similarity(head_b)\n",
    "\n",
    "                head = 0 if head is None else head  \n",
    "\n",
    "            except Exception:\n",
    "                head = 0\n",
    "\n",
    "        if dep_a == dep_b:\n",
    "            dep = 1\n",
    "        else:\n",
    "            try:\n",
    "                #WordNet synsets for dependent\n",
    "                dep_a, dep_b = wn.synsets(dep_a)[0], wn.synsets(dep_b)[0]\n",
    "                \n",
    "#                 TODO:Change similarity method\n",
    "                #path based similarity (Li et. al) for dependent\n",
    "                dep = dep_a.path_similarity(dep_b)\n",
    "\n",
    "                dep = 0 if dep is None else dep\n",
    "\n",
    "            except Exception:\n",
    "                dep = 0     \n",
    "\n",
    "        return head + dep\n",
    "\n",
    "    def _similarity_tag(self, tag_a, tag_b):\n",
    "        \n",
    "        tag_a_id, tag_b_id = self._tags[tag_a], self._tags[tag_b] \n",
    "        score = self._tag_correlation_matrix[tag_a_id,tag_b_id]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def semantic_similarity(self, documents_1, documents_2):\n",
    "        \n",
    "        #checking the sizes of both documents\n",
    "        assert len(documents_1) == len(documents_2), \"Size of both lists should be same.\"\n",
    "        \n",
    "        #scores vector\n",
    "        scores = torch.zeros([len(documents_1),],dtype=torch.double)\n",
    "        \n",
    "        i = 0\n",
    "            \n",
    "        for document_1, document_2 in zip(documents_1,documents_2):\n",
    "            \n",
    "            #parsing documets using spaCy English language parser\n",
    "            tokens_1,tokens_2 = self._parser(document_1), self._parser(document_2)\n",
    "\n",
    "            #seperating dependency pairs and tags from tokens\n",
    "            pairs_1 = [(token.head,token.dep_,token) for token in tokens_1]\n",
    "            pairs_2 = [(token.head,token.dep_,token) for token in tokens_2]\n",
    "\n",
    "            score = 0\n",
    "\n",
    "            #calculating score \n",
    "            for pair_A in pairs_1:\n",
    "\n",
    "                for pair_B in pairs_2:\n",
    "\n",
    "                    score += self._similarity_word(pair_A, pair_B) * self._similarity_tag(pair_A[1], pair_B[1])\n",
    "\n",
    "            #averaging score \n",
    "            #score = score / (len(tokens_1) + len(tokens_2))\n",
    "            \n",
    "            score = torch.sigmoid(score)\n",
    "            \n",
    "            scores[i] += score\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        return scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], dtype=torch.float64)\n",
      "tensor([0.9943, 0.9945], dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sim = TextSimilarity()\n",
    "sim._tag_correlation_matrix = torch.from_numpy(sim._tag_correlation_matrix)\n",
    "print(sim._tag_correlation_matrix )\n",
    "sim._tag_correlation_matrix.requires_grad = True\n",
    "sent_1 = [\"he is boy\",\"it is dog\"]\n",
    "sent_2 = [\"he is girl\",\"it is cat\"]\n",
    "score = sim.semantic_similarity(sent_1,sent_2)\n",
    "print(score)\n",
    "print(sim._tag_correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 44])\n"
     ]
    }
   ],
   "source": [
    "print(sim._tag_correlation_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9943, 0.9945], dtype=torch.float64, grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06225100073703524\n",
      "tensor([[0.0054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9990, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSELoss()\n",
    "optimizer = torch.optim.Adam([sim._tag_correlation_matrix], lr=0.001)\n",
    "target = torch.DoubleTensor([.8,.7])\n",
    "target.requires_grad = True\n",
    "loss_score = loss(score, target)\n",
    "print(loss_score.item())\n",
    "loss_score.backward()\n",
    "print(sim._tag_correlation_matrix.grad)\n",
    "optimizer.step()\n",
    "sim._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9943, 0.9944], dtype=torch.float64, grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = sim.semantic_similarity(sent_1,sent_2)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9990, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, epochs = 25, lr = 0.001, validation_thresh = 0.2, batch_size = 10, print_every = 5):\n",
    "    \n",
    "    model._tag_correlation_matrix = torch.from_numpy(model._tag_correlation_matrix)\n",
    "    model._tag_correlation_matrix.requires_grad = True\n",
    "    \n",
    "    criterion = MSELoss()\n",
    "    optimizer = torch.optim.Adam([model._tag_correlation_matrix], lr=lr)\n",
    "    \n",
    "    thresh = int(validation_thresh * len(data))\n",
    "    \n",
    "    train_data = data.iloc[:thresh]\n",
    "    valid_data = data.iloc[thresh:]\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    count = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for x1, x2, Y in get_batches(train_data, batch_size):\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "            scores = model.semantic_similarity(x1, x2)\n",
    "            \n",
    "            Y = torch.DoubleTensor(Y)\n",
    "            \n",
    "            loss = criterion(scores, Y)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            if count % print_every == 0:\n",
    "                \n",
    "                train_losses.append(np.mean(losses))\n",
    "                losses = []\n",
    "                \n",
    "                for x1, x2, Y in get_batches(valid_data, batch_size):\n",
    "                    \n",
    "                    scores = model.semantic_similarity(x1, x2)\n",
    "                    \n",
    "                    Y = torch.DoubleTensor(Y)\n",
    "                    \n",
    "                    loss = criterion(scores, Y)\n",
    "\n",
    "                    losses.append(loss.item())\n",
    "                    \n",
    "                valid_losses.append(np.mean(losses))\n",
    "                \n",
    "                print(f\"{count} {epoch}/{epochs}\\ttraining loss:{train_losses[-1]}\\tvalididation loss:{valid_losses[-1]}\")\n",
    "    \n",
    "    return train_losses,valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The problem likely will mean corrective change...</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technology-laced Nasdaq Composite Index .I...</td>\n",
       "      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n",
       "      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SEC Chairman William Donaldson said there is a...</td>\n",
       "      <td>\"I think there's a building confidence that th...</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n",
       "      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Myanmar's pro-democracy leader Aung San Suu Ky...</td>\n",
       "      <td>Myanmar's pro-democracy leader Aung San Suu Ky...</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Darren Dopp, a Spitzer spokesman, declined to ...</td>\n",
       "      <td>John Heine, a spokesman for the commission in ...</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Bremer said one initiative is to launch a US$7...</td>\n",
       "      <td>Bremer said he would launch a $70-million prog...</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>In afternoon trading in Europe, France's CAC-4...</td>\n",
       "      <td>In Europe, France's CAC-40 rose 1.3 percent, B...</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>He later learned that the incident was caused ...</td>\n",
       "      <td>He later found out the alarming incident had b...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0         0.0   \n",
       "1             1         1.0   \n",
       "2             2         2.0   \n",
       "3             3         3.0   \n",
       "4             4         4.0   \n",
       "5             5         5.0   \n",
       "6             6         6.0   \n",
       "7             7         7.0   \n",
       "8             8         8.0   \n",
       "9             9         9.0   \n",
       "\n",
       "                                              sent_1  \\\n",
       "0  The problem likely will mean corrective change...   \n",
       "1  The technology-laced Nasdaq Composite Index .I...   \n",
       "2  \"It's a huge black eye,\" said publisher Arthur...   \n",
       "3  SEC Chairman William Donaldson said there is a...   \n",
       "4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n",
       "5  Myanmar's pro-democracy leader Aung San Suu Ky...   \n",
       "6  Darren Dopp, a Spitzer spokesman, declined to ...   \n",
       "7  Bremer said one initiative is to launch a US$7...   \n",
       "8  In afternoon trading in Europe, France's CAC-4...   \n",
       "9  He later learned that the incident was caused ...   \n",
       "\n",
       "                                              sent_2  score  \n",
       "0  He said the problem needs to be corrected befo...   0.88  \n",
       "1  The broad Standard & Poor's 500 Index .SPX inc...   0.16  \n",
       "2  \"It's a huge black eye,\" Arthur Sulzberger, th...   0.72  \n",
       "3  \"I think there's a building confidence that th...   0.68  \n",
       "4  In New York, Vivendi shares were 1.4 percent d...   0.28  \n",
       "5  Myanmar's pro-democracy leader Aung San Suu Ky...   0.92  \n",
       "6  John Heine, a spokesman for the commission in ...   0.28  \n",
       "7  Bremer said he would launch a $70-million prog...   0.72  \n",
       "8  In Europe, France's CAC-40 rose 1.3 percent, B...   0.40  \n",
       "9  He later found out the alarming incident had b...   1.00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/STS_Data1213.csv\",index_col=None)\n",
    "data.score = data.score/5.0\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextSimilarity()\n",
    "thresh = int(0.8 * len(data))\n",
    "\n",
    "train_data = data.iloc[:thresh]\n",
    "test_data = data.iloc[thresh:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4/200\ttraining loss:0.1577291829401076\tvalididation loss:0.1783375198588286\n",
      "10 9/200\ttraining loss:0.15728344917304762\tvalididation loss:0.1804730303276525\n",
      "15 14/200\ttraining loss:0.15538885139573352\tvalididation loss:0.19271314876798834\n",
      "20 19/200\ttraining loss:0.14856060028679688\tvalididation loss:0.20855987783362778\n",
      "25 24/200\ttraining loss:0.1333563197057729\tvalididation loss:0.219487517058085\n",
      "30 29/200\ttraining loss:0.10828713656480278\tvalididation loss:0.22098701012706126\n",
      "35 34/200\ttraining loss:0.11455359711914002\tvalididation loss:0.22071769711162562\n",
      "40 39/200\ttraining loss:0.07087490610687906\tvalididation loss:0.2198690019388132\n",
      "45 44/200\ttraining loss:0.07130044812533702\tvalididation loss:0.21873308322344404\n",
      "50 49/200\ttraining loss:0.05210152827839847\tvalididation loss:0.21800020169044654\n",
      "55 54/200\ttraining loss:0.02236801281699485\tvalididation loss:0.2176876986988361\n",
      "60 59/200\ttraining loss:0.02122762508178003\tvalididation loss:0.21734036564204004\n",
      "65 64/200\ttraining loss:0.023088648559967416\tvalididation loss:0.2178289598592541\n",
      "70 69/200\ttraining loss:0.021415662488470027\tvalididation loss:0.2172976359316887\n",
      "75 74/200\ttraining loss:0.018897319335729707\tvalididation loss:0.21751604975660493\n",
      "80 79/200\ttraining loss:0.01787307525969146\tvalididation loss:0.21749479571841374\n",
      "85 84/200\ttraining loss:0.017477182603232062\tvalididation loss:0.21723402968111083\n",
      "90 89/200\ttraining loss:0.017490764005072438\tvalididation loss:0.2173534844488334\n",
      "95 94/200\ttraining loss:0.017036826895054234\tvalididation loss:0.21727552209854334\n",
      "100 99/200\ttraining loss:0.01644780909212621\tvalididation loss:0.2170944304093414\n",
      "105 104/200\ttraining loss:0.015820005540731492\tvalididation loss:0.21697154129193688\n",
      "110 109/200\ttraining loss:0.014357813799580735\tvalididation loss:0.21664088080420346\n",
      "115 114/200\ttraining loss:0.010985981338821674\tvalididation loss:0.21605378045919016\n",
      "120 119/200\ttraining loss:0.003457113744307395\tvalididation loss:0.21499018266117387\n",
      "125 124/200\ttraining loss:0.0013249165049585164\tvalididation loss:0.21383540128492085\n",
      "130 129/200\ttraining loss:0.0017160753610010043\tvalididation loss:0.21403012392523854\n",
      "135 134/200\ttraining loss:0.00027823272333204324\tvalididation loss:0.2148204747059458\n",
      "140 139/200\ttraining loss:0.0007942399023984917\tvalididation loss:0.21492154769573282\n",
      "145 144/200\ttraining loss:7.609648202475031e-05\tvalididation loss:0.2144888689688754\n",
      "150 149/200\ttraining loss:0.00023445030827117373\tvalididation loss:0.2142417633020099\n",
      "155 154/200\ttraining loss:6.594590284352084e-05\tvalididation loss:0.21444077188037985\n",
      "160 159/200\ttraining loss:5.653132216226636e-05\tvalididation loss:0.21463869596002982\n",
      "165 164/200\ttraining loss:4.0595803667912154e-05\tvalididation loss:0.21456469094367034\n",
      "170 169/200\ttraining loss:1.0604563256712954e-05\tvalididation loss:0.21442807880628087\n",
      "175 174/200\ttraining loss:1.8387574926111454e-05\tvalididation loss:0.214439989980599\n",
      "180 179/200\ttraining loss:2.161813660344056e-06\tvalididation loss:0.21452499179720805\n",
      "185 184/200\ttraining loss:7.353113374218706e-06\tvalididation loss:0.21453523145571946\n",
      "190 189/200\ttraining loss:4.0709103619667986e-07\tvalididation loss:0.21448359984938234\n",
      "195 194/200\ttraining loss:2.7401896331281396e-06\tvalididation loss:0.21447010699665\n",
      "200 199/200\ttraining loss:5.668747669287207e-08\tvalididation loss:0.21449887584735525\n"
     ]
    }
   ],
   "source": [
    "#TODO: change epoch, lr, batch_size\n",
    "epoch = 200\n",
    "lr = 0.01\n",
    "batch_size = 10\n",
    "\n",
    "train_losses,valid_losses = train(train_data, model, epochs=epoch, lr=lr, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7640,  0.0000, -0.3066,  ..., -0.1067, -0.2101, -0.2123],\n",
       "        [ 0.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.2917,  0.0000,  0.7024,  ...,  0.0000,  0.0000, -0.2678],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n",
       "        [-0.2101,  0.0000,  0.0000,  ...,  0.0000,  0.7338, -0.2069],\n",
       "        [-0.2779,  0.0000, -0.2679,  ...,  0.0000, -0.2069,  0.7140]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._tag_correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Y = test_data.score\n",
    "scores = model.semantic_similarity(test_data.sent_1, test_data.sent_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f673e5a2b50>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses)\n",
    "plt.plot(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2603, dtype=torch.float64, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y = torch.DoubleTensor(test_Y.values)\n",
    "loss(scores,test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
